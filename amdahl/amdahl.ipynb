{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lei de Amdahl\n",
    "\n",
    "__Introdução__\n",
    "\n",
    "O que se conhece hoje como **lei de Amdahl**, é o resultado das análises realizadas e publicadas pelo Dr. Gene Amdahl em 1967. Nestas análises, mesmo sem propor equação alguma, Amdahl apresenta contraargumentos à idéia de que a computação paralela era a melhor alternativa, na época, para se alcançar altos desempenhos na computação, demonstrando a existência de limites teóricos referentes à máxima aceleração que pode obter-se através da utilização de arquiteturas paralelas.\n",
    "\n",
    "Dentre os argumentos apresentados  como limitantes dos possíveis ganhos a parte de gerenciamento de tarefas(referida como \"housekeeping management\"), que à época era responsável por cerca de 40% do tempo de execução de algoritmos (podendo ser reduzido pela metade, ou até próximo de 1/3, mas nunca chegando a esse valor, com o uso de ambientes dedicados), é apresentada como o principal fator. Contornos irregulares, interiores não-homogêneos e aumento da complexidade do gerenciamento de dados também são apontados como limitantes importantes. [1]  \n",
    "\n",
    "\n",
    "__Relação entre lei de Amdahl e Speedup__\n",
    "\n",
    "Uma vez que Amdahl não traz fórmulas ou equações para auxiliar a visualização do limite máximo proposto para o ganho em tempo de processamento que é possível de se obter através da paralelização de um programa, uma das opções é realizar o cálculo do speedup teórico.\n",
    "\n",
    "> Speedup é uma **métrica de desempenho relativo** entre os tempos de execução de dois algoritmos iguais, mas executados por sistemas diferentes, ou quando alguma técnica de otimização ou paralelização é aplicada a um deles.\n",
    "\n",
    "No caso da programação paralela, o speedup leva em consideração a existência de uma parte do algoritmo que pode ser paralelizada, e compara o tempo de execução serial com o tempo de execução com paralelização. Formalizando, temos a seguinte equação:\n",
    "\n",
    "$$S(n) = \\frac{1}{f + \\frac{1-f}{n}}$$\n",
    "Onde:\n",
    "* S(n) é o speedup quando utiliza-se n processadores para a execução de um algoritmo\n",
    "* n é o número de processadores utilizados\n",
    "* f é a fração do algoritmo que é estritamentre sequencial\n",
    "\n",
    "Em um contexto histórico no qual a computação paralela era apontada como uma das melhores soluções para o aumento do poder de processamento até então disponível (uma vez que as limitações eram tecnológicas), era notória a impossibilidade conseguir um ganho equivalente ao número de novos processadores adicionados para realizar uma tarefa, afinal sempre existirá uma parte do programa que não tem como dividir (paralelizar) denominada **fração estritamente sequencial** do algoritmo. Amdahl propôs que o ganho em eficiência era limitado justamente pela existência de tal parte. Analisando a fórmula do speedup, pode-se chegar a essa memsma conclusão. Para isso, imagine uma situação hipotética em que infinitos processadores sejam utilizados, e 50% do código possa ser paralelizado. Nesse cenário, o limite do speedup seria:\n",
    "\n",
    "$$ S(\\infty) = \\frac{1}{0.5 + \\frac{1-0.5}{\\infty}} = 2$$\n",
    "\n",
    "Fazendo o mesmo cálculo para os valores f = 25%, f = 10% e f = 5%, chegamos aos máximos speedups teóricos como sendo S = 4, S = 10 e S = 20, respectivamente. O gráfico abaixo representa os resultados desses cálculos.\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/AmdahlsLaw.svg/640px-AmdahlsLaw.svg.png)\n",
    "<p style=text-align:center> **Fonte:** Wikipedia[4] </p>\n",
    "\n",
    "No entanto, somente a fórmula do Speedup não é suficiente para descrever todos os fatores envolvidos no tempo de processamento de um algoritmo. Diversos outros fatores conhecidos como **gargalos** (bottlenecks, em inglês) são responsáveis por perdas que afetam a velocidade de processamento[3]. Tais fatores incluem, mas não se limitam a:\n",
    "* Arquitetura da(s) máquina(s)\n",
    "* Problemas de Sincronização (quando dois processos buscam modificar a mesma informação ao mesmo tempo)\n",
    "* Compartilhamento de recursos (Cache, por exemplo)\n",
    "* Desbalanceamento de carga\n",
    "\n",
    "Outro ponto importante, e que foi também citado por Amdahl é o fato de que o custo de máquinas tende a crescer de maneira mais acentuada que o rendimento obtido, e esse custo tende a ser ainda maior para a obtenção / construção de arquiteturas altamente paralelas.\n",
    "\n",
    "\n",
    "**Speedup supralinear e a lei de Amdahl**\n",
    "\n",
    "Em algumas literaturas existe a menção ao speedup supralinear como uma contradição à lei de Amdahl. Porém, é preciso observar que para se alcançar speedups supralineares uma análise das condições utilizadas para a obtenção de tal performance. Em geral este nível de desempenho é alcançado somente para problemas específicos (como problemas que envolvem buscas, por exemplo), e que apresentam uma utilização de recuros mais eficiente, o que não foi consideado por Amdahl à época da divulgação de suas análises.\n",
    "\n",
    "**Referências**\n",
    "\n",
    "[1] https://sscs.ieee.org/images/files/newsletter_archive/sscs_newsletter_200707.pdf\n",
    "\n",
    "[2] http://www.di-srv.unisa.it/~vitsca/SC-2011/DesignPrinciplesMulticoreProcessors/Krishnaprasad2001.pdf\n",
    "\n",
    "[3] http://users.elis.ugent.be/~leeckhou/papers/ispass12_2.pdf\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Amdahl%27s_law"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
